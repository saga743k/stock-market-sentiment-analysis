# -*- coding: utf-8 -*-
"""nlp project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16kxuGbYCe0LnlN53OAL5KoUVBYzVESlU
"""

import tensorflow as tf
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import nltk
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import re
import random
import seaborn as sns
import requests
import zipfile
import os

nltk.download('stopwords')

print("Tensorflow Version", tf.__version__)





# Load the dataset
df = pd.read_csv('stock_data.csv', encoding='latin', header=None)
df1 = df.iloc[1:]
df.drop(index=df.index[0], axis=0, inplace=True)
df.columns = ['text', 'sentiment']

# Label decoding
lab_to_sentiment = {"-1": "Negative", "1": "Positive"}
def label_decoder(label):
    y = lab_to_sentiment.get(label)
    if y is not None:
        return y
    else:
        return "Unknown"

df.sentiment = df.sentiment.apply(lambda x: label_decoder(x))

# Display first few rows
df.head()

# Sentiment data distribution plot
val_count = df.sentiment.value_counts()
plt.figure(figsize=(8, 4))
sns.barplot(x=val_count.index, y=val_count.values, hue=val_count.index, palette="viridis", dodge=False, legend=False)
plt.title("Sentiment Data Distribution")
plt.xlabel('Sentiment')
plt.ylabel('Count')
plt.show()

# Display random samples
random_idx_list = [random.randint(1, len(df.text)) for _ in range(10)]
df.loc[random_idx_list, :].head(10)

# Define stopwords and stemmer
stop_words = stopwords.words('english')
stemmer = SnowballStemmer('english')

# Preprocessing function
text_cleaning_re = "@\S+|https?:\S+|http?:\S|[^A-Za-z0-9]+"
def preprocess(text, stem=False):
    text = re.sub(text_cleaning_re, ' ', str(text).lower()).strip()
    tokens = []
    for token in text.split():
        if token not in stop_words:
            if stem:
                tokens.append(stemmer.stem(token))
            else:
                tokens.append(token)
    return " ".join(tokens)

df.text = df.text.apply(lambda x: preprocess(x))

# Display first few rows after preprocessing
df.head()

# Plot text length distribution
df['text_length'] = df.text.apply(lambda x: len(x.split()))
plt.figure(figsize=(12, 6))
sns.histplot(df['text_length'], kde=True, bins=30, color='blue')
plt.title("Text Length Distribution")
plt.show()

# Plot the count of each sentiment
plt.figure(figsize=(8, 4))
sns.countplot(x='sentiment', data=df, hue='sentiment', palette='viridis', dodge=False, legend=False)
plt.title('Sentiment Count')
plt.xlabel('Sentiment')
plt.ylabel('Count')
plt.show()

# Plot sentiment distribution by text length
plt.figure(figsize=(12, 6))
sns.boxplot(x='sentiment', y='text_length', data=df, hue='sentiment', palette="viridis", legend=False)
plt.title("Sentiment Distribution by Text Length")
plt.show()

from collections import Counter

# Function to plot the most common words
def plot_most_common_words(text, sentiment):
    words = ' '.join(text).split()
    word_freq = Counter(words)
    common_words = word_freq.most_common(20)

    df_common_words = pd.DataFrame(common_words, columns=['word', 'count'])

    plt.figure(figsize=(12, 6))
    sns.barplot(x='count', y='word', hue='word', dodge=False, data=df_common_words, palette='viridis', legend=False)
    plt.title(f'Most Common Words in {sentiment} Texts')
    plt.show()

# Plot most common words for positive texts
plot_most_common_words(df[df.sentiment == 'Positive'].text, 'Positive')

# Plot most common words for negative texts
plot_most_common_words(df[df.sentiment == 'Negative'].text, 'Negative')

from sklearn.feature_extraction.text import CountVectorizer

# Function to plot top n-grams
def plot_top_ngrams(text, n, title):
    vec = CountVectorizer(ngram_range=(n, n)).fit(text)
    bag_of_words = vec.transform(text)
    sum_words = bag_of_words.sum(axis=0)
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)

    df_ngrams = pd.DataFrame(words_freq[:20], columns=['word', 'count'])

    plt.figure(figsize=(12, 6))
    sns.barplot(x='count', y='word', hue='word', dodge=False, data=df_ngrams, palette='coolwarm', legend=False)
    plt.title(f'Top {n}-grams')
    plt.show()

# Plot top unigrams (1-grams)
plot_top_ngrams(df.text, 1, 'Top Unigrams')

# Plot top bigrams (2-grams)
plot_top_ngrams(df.text, 2, 'Top Bigrams')

# Plot top trigrams (3-grams)
plot_top_ngrams(df.text, 3, 'Top Trigrams')

# Distribution of text length by sentiment
plt.figure(figsize=(12, 6))
sns.histplot(data=df, x='text_length', hue='sentiment', multiple='stack', palette='viridis')
plt.title('Distribution of Text Length by Sentiment')
plt.xlabel('Text Length')
plt.ylabel('Count')
plt.show()

# Boxen plot of text length by sentiment
plt.figure(figsize=(12, 6))
sns.boxenplot(x='sentiment', y='text_length', hue='sentiment', palette='viridis', dodge=False, legend=False, data=df)
plt.title('Distribution of Text Length by Sentiment', fontsize=16)
plt.xlabel('Sentiment', fontsize=14)
plt.ylabel('Text Length', fontsize=14)
plt.show()

TRAIN_SIZE = 0.8
MAX_SEQUENCE_LENGTH = 30

# Split the data
train_data, test_data = train_test_split(df, test_size=1-TRAIN_SIZE, random_state=7)
print("Train Data size:", len(train_data))
print("Test Data size:", len(test_data))

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Tokenize the text
tokenizer = Tokenizer()
tokenizer.fit_on_texts(train_data.text)

word_index = tokenizer.word_index
vocab_size = len(tokenizer.word_index) + 1
print("Vocabulary Size:", vocab_size)

# Pad the sequences
x_train = pad_sequences(tokenizer.texts_to_sequences(train_data.text), maxlen=MAX_SEQUENCE_LENGTH)
x_test = pad_sequences(tokenizer.texts_to_sequences(test_data.text), maxlen=MAX_SEQUENCE_LENGTH)

print("Training X Shape:", x_train.shape)
print("Testing X Shape:", x_test.shape)

# Encode the labels
labels = train_data.sentiment.unique().tolist()
encoder = LabelEncoder()
encoder.fit(train_data.sentiment.to_list())

y_train = encoder.transform(train_data.sentiment.to_list())
y_test = encoder.transform(test_data.sentiment.to_list())

y_train = y_train.reshape(-1, 1)
y_test = y_test.reshape(-1, 1)

print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)

# Path to save the GloVe zip file
zip_path = os.path.join("Documents", "Deep Learning", "NLP", "stocks", "glove.6B.zip")
extracted_path = os.path.join("Documents", "Deep Learning", "NLP", "stocks")

# URL of the GloVe embeddings
url = "http://nlp.stanford.edu/data/glove.6B.zip"

# Create directories if they don't exist
os.makedirs(extracted_path, exist_ok=True)

# Download the file
response = requests.get(url, stream=True)
with open(zip_path, "wb") as f:
    for chunk in response.iter_content(chunk_size=128):
        f.write(chunk)

# Unzip the file
with zipfile.ZipFile(zip_path, "r") as zip_ref:
    zip_ref.extractall(extracted_path)

# Path to the extracted GloVe file
GLOVE_EMB = os.path.join(extracted_path, "glove.6B.300d.txt")
if not os.path.exists(GLOVE_EMB):
    raise FileNotFoundError(f"File not found: {GLOVE_EMB}")

# Load the GloVe embeddings
embeddings_index = {}

with open(GLOVE_EMB, 'r', encoding='utf-8') as f:
    for line in f:
        values = line.split()
        word = values[0]
        coefs = np.asarray(values[1:], dtype='float32')
        embeddings_index[word] = coefs

print('Found %s word vectors.' % len(embeddings_index))

# Create the embedding matrix
EMBEDDING_DIM = 300
embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))
for word, i in word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector

# Create the embedding layer
embedding_layer = tf.keras.layers.Embedding(vocab_size,
                                            EMBEDDING_DIM,
                                            weights=[embedding_matrix],
                                            trainable=False)

from tensorflow.keras.layers import Conv1D, Bidirectional, LSTM, Dense, Input, Dropout, SpatialDropout1D
from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam

# Define additional parameters
LR = 1e-3
BATCH_SIZE = 1024
EPOCHS = 10

# Define the model
sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')
embedding_sequences = embedding_layer(sequence_input)
x = SpatialDropout1D(0.2)(embedding_sequences)
x = Conv1D(64, 5, activation='relu')(x)
x = Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2))(x)
x = Dense(512, activation='relu')(x)
x = Dropout(0.5)(x)
x = Dense(512, activation='relu')(x)
outputs = Dense(1, activation='sigmoid')(x)
model = tf.keras.Model(sequence_input, outputs)

# Compile the model
model.compile(optimizer=Adam(learning_rate=LR), loss='binary_crossentropy', metrics=['accuracy'])

# Define callbacks
reduce_lr = ReduceLROnPlateau(factor=0.1, min_lr=0.01, monitor='val_loss', verbose=1)

# Train the model
print("Training on GPU...") if tf.test.is_gpu_available() else print("Training on CPU...")

history = model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS,
                    validation_data=(x_test, y_test), callbacks=[reduce_lr])

# Plot training history
s, (at, al) = plt.subplots(2, 1, figsize=(10, 10))
at.plot(history.history['accuracy'], c='brown')
at.plot(history.history['val_accuracy'], c='pink')
at.set_title('Model Accuracy')
at.set_ylabel('Accuracy')
at.set_xlabel('Epoch')
at.legend(['Train', 'Validation'], loc='upper left')

al.plot(history.history['loss'], c='purple')
al.plot(history.history['val_loss'], c='cyan')
al.set_title('Model Loss')
al.set_ylabel('Loss')
al.set_xlabel('Epoch')
al.legend(['Train', 'Validation'], loc='upper left')

plt.tight_layout()
plt.show()

# Decode sentiment function
def decode_sentiment(score):
    return "Positive" if score > 0.5 else "Negative"

# Predict and decode sentiment
scores = model.predict(x_test, verbose=1, batch_size=10000)
y_pred_1d = [decode_sentiment(score) for score in scores]

import itertools
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score

# Plot confusion matrix
def plot_confusion_matrix(cm, classes, title='Confusion matrix', cmap=plt.cm.Oranges):
    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title, fontsize=20)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, fontsize=13)
    plt.yticks(tick_marks, classes, fontsize=13)

    fmt = '.2f'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.ylabel('True label', fontsize=17)
    plt.xlabel('Predicted label', fontsize=17)

# Compute and plot confusion matrix
cnf_matrix = confusion_matrix(test_data.sentiment.to_list(), y_pred_1d)
plt.figure(figsize=(6, 6))
plot_confusion_matrix(cnf_matrix, classes=test_data.sentiment.unique(), title="Confusion matrix")
plt.show()

from sklearn.metrics import roc_curve, auc

# Compute ROC curve and ROC area
fpr, tpr, _ = roc_curve(y_test, scores)
roc_auc = auc(fpr, tpr)

# Plot ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:0.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate', fontsize=14)
plt.ylabel('True Positive Rate', fontsize=14)
plt.title('Receiver Operating Characteristic (ROC) Curve', fontsize=16)
plt.legend(loc="lower right")
plt.show()

from sklearn.metrics import precision_recall_curve, average_precision_score

# Compute precision-recall curve and average precision
precision, recall, _ = precision_recall_curve(y_test, scores)
average_precision = average_precision_score(y_test, scores)

# Plot Precision-Recall curve
plt.figure(figsize=(8, 6))
plt.plot(recall, precision, color='blue', lw=2, label=f'Precision-Recall curve (area = {average_precision:0.2f})')
plt.xlabel('Recall', fontsize=14)
plt.ylabel('Precision', fontsize=14)
plt.title('Precision-Recall Curve', fontsize=16)
plt.legend(loc="lower left")
plt.show()

# Plot distribution of predicted probabilities
plt.figure(figsize=(12, 6))
sns.histplot(scores, bins=50, color='purple', kde=True)
plt.title('Distribution of Predicted Probabilities', fontsize=16)
plt.xlabel('Predicted Probability of Positive', fontsize=14)
plt.ylabel('Count', fontsize=14)
plt.show()

# Get user input for a news headline
user_input = input("Enter a news headline: ")

# Create a DataFrame with the user's input
user_df = pd.DataFrame({'text': [user_input]})

# Define the max_length (make sure this is the same as in your training)
max_length = 100

# Apply the same preprocessing and tokenization as your training data
user_sequences = tokenizer.texts_to_sequences(user_df['text'])
user_padded = pad_sequences(user_sequences, maxlen=max_length, padding='post', truncating='post')

# Predict the sentiment of the input
prediction = model.predict(user_padded)
score = prediction[0][0]

# --- MODIFICATION ---
# Print the raw score to see what the model is outputting
print(f"\nRaw Prediction Score: {score:.4f}")


# You can now experiment with this threshold
# For example, if your model is biased towards positive, you might need a higher threshold
custom_threshold = 0.58

# Display the prediction based on the custom threshold
if score > custom_threshold:
    print(f"Sentiment: Positive (Confidence Score: {score:.2f})")
else:
    print(f"Sentiment: Negative (Confidence Score: {1 - score:.2f})")

import tkinter as tk
from tkinter import messagebox

# -------- Your existing functions & model must be loaded above this -------- #

# preprocess() function (Already created in your code)
# tokenizer (Already created)
# model (Already trained & loaded)
# max_length = 100 (same as your code)


# Prediction function for UI
def predict_sentiment():
    headline = entry.get()

    if headline.strip() == "":
        messagebox.showwarning("Input Error", "Please enter a news headline!")
        return

    # Preprocess
    seq = tokenizer.texts_to_sequences([headline])
    padded = pad_sequences(seq, maxlen=max_length, padding='post', truncating='post')

    # Predict
    pred = model.predict(padded)[0][0]

    custom_threshold = 0.58  # your custom threshold

    if pred > custom_threshold:
        sentiment = "Positive"
        confidence = pred
    else:
        sentiment = "Negative"
        confidence = 1 - pred

    # Display output
    result_label.config(
        text=f"Sentiment: {sentiment}\nConfidence: {confidence:.2f}",
        fg="green" if sentiment == "Positive" else "red",
        font=("Arial", 14, "bold")
    )


# ----------- Tkinter Window ----------- #
root = tk.Tk()
root.title("Stock News Sentiment Analyzer")
root.geometry("600x400")
root.config(bg="#E8F0FE")

title = tk.Label(root, text="Stock Sentiment Analysis", font=("Arial", 22, "bold"), bg="#E8F0FE")
title.pack(pady=20)

entry_label = tk.Label(root, text="Enter News Headline:", font=("Arial", 14), bg="#E8F0FE")
entry_label.pack()

entry = tk.Entry(root, width=60, font=("Arial", 12))
entry.pack(pady=10)

predict_btn = tk.Button(root, text="Predict Sentiment", font=("Arial", 14), bg="#4C8BF5", fg="white",
                        command=predict_sentiment)
predict_btn.pack(pady=20)

result_label = tk.Label(root, text="", font=("Arial", 14), bg="#E8F0FE")
result_label.pack(pady=20)

root.mainloop()

from ipywidgets import widgets, VBox, HBox, HTML
from IPython.display import display, clear_output

# Dummy functions -- replace with your own
def predict_sentiment(text):
    return 0.6

def get_news_for_stock(stock):
    return [
        {"title": f"{stock} surges in market rally", "description": "Investors show confidence."},
        {"title": f"{stock} CEO addresses concerns", "description": "Analysts remain uncertain."},
        {"title": f"{stock} new product launch", "description": "Market response mixed."},
    ]

# ---------------------- UI COMPONENTS ---------------------- #

title = HTML(
    value="<h1 style='text-align:center; color:#2C3E50; font-size:40px;'>üìà Stock News Sentiment Dashboard</h1>"
)

stock_input = widgets.Text(
    description="",
    placeholder="Enter stock ticker or company name",
    layout=widgets.Layout(width="60%", height="40px")
)

search_btn = widgets.Button(
    description="Analyze Sentiment",
    button_style="success",
    layout=widgets.Layout(width="30%", height="40px")
)

output_area = widgets.HTML(
    value="",
    layout=widgets.Layout(
        border="1px solid #ccc",
        padding="20px",
        width="100%",
        height="450px",
        overflow_y="scroll"
    )
)

# ---------------------- CALLBACK FUNCTION ---------------------- #

def analyze_news(b):
    stock = stock_input.value.strip()

    if not stock:
        output_area.value = "<h3 style='color:red;'>‚ùó Please enter a stock name.</h3>"
        return

    news_items = get_news_for_stock(stock)

    html_content = f"<h2 style='color:#34495E;'>Results for: <b>{stock}</b></h2><br>"

    for article in news_items:
        text = article['title'] + " " + article['description']
        score = predict_sentiment(text)

        # sentiment tag
        if score > 0.5:
            tag = "<span style='background:#2ECC71; padding:6px 12px; border-radius:8px; color:white;'>Positive</span>"
        elif score < -0.5:
            tag = "<span style='background:#E74C3C; padding:6px 12px; border-radius:8px; color:white;'>Negative</span>"
        else:
            tag = "<span style='background:#F1C40F; padding:6px 12px; border-radius:8px; color:white;'>Neutral</span>"

        # card-style display
        html_content += f"""
            <div style='border:1px solid #ddd; border-radius:10px; padding:15px; margin-bottom:15px; background:#FAFAFA;'>
                <h3 style='margin:0; color:#2C3E50;'>{article['title']}</h3>
                <p style='color:#7F8C8D;'>{article['description']}</p>
                <p><b>Sentiment Score:</b> {score:.2f} {tag}</p>
            </div>
        """

    output_area.value = html_content

search_btn.on_click(analyze_news)

# Layout row for input
input_row = HBox(
    [stock_input, search_btn],
    layout=widgets.Layout(justify_content="center", margin="20px")
)

# Final layout
ui = VBox([title, input_row, output_area])
display(ui)

import tkinter as tk
from tkinter import messagebox
import numpy as np
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.sequence import pad_sequences
import pickle
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer

# -----------------------------
# Load Saved Model & Tokenizer
# -----------------------------
model = load_model("sentiment_model.h5")

with open("tokenizer.pkl", "rb") as f:
    tokenizer = pickle.load(f)

# -----------------------------
# Preprocessing Function
# -----------------------------
stop_words = stopwords.words('english')
stemmer = SnowballStemmer('english')
text_cleaning_re = r"@\S+|https?:\S+|http?:\S|[^A-Za-z0-9]+"

def preprocess(text, stem=False):
    text = re.sub(text_cleaning_re, ' ', text.lower()).strip()
    tokens = []
    for token in text.split():
        if token not in stop_words:
            tokens.append(stemmer.stem(token) if stem else token)
    return " ".join(tokens)

# -----------------------------
# Predict Function
# -----------------------------
def predict_sentiment():
    headline = input_box.get("1.0", "end-1c").strip()

    if headline == "":
        messagebox.showwarning("Warning", "Please enter a headline!")
        return

    clean_text = preprocess(headline)
    seq = tokenizer.texts_to_sequences([clean_text])
    padded = pad_sequences(seq, maxlen=100, padding='post')

    score = model.predict(padded)[0][0]
    custom_threshold = 0.5
    sentiment = "Positive" if score > custom_threshold else "Negative"
    confidence = score if score > custom_threshold else 1 - score

    result_label.config(
        text=f"Sentiment: {sentiment}\nConfidence: {confidence:.2f}",
        fg="green" if sentiment == "Positive" else "red",
    )

# -----------------------------
# UI Window (Tkinter)
# -----------------------------
window = tk.Tk()
window.title("Stock News Sentiment Analyzer")
window.geometry("500x400")
window.config(bg="#f2f2f2")

title_label = tk.Label(window, text="Stock Sentiment Analyzer",
                       font=("Arial", 20, "bold"), bg="#f2f2f2")
title_label.pack(pady=10)

input_label = tk.Label(window, text="Enter News Headline:",
                       font=("Arial", 12), bg="#f2f2f2")
input_label.pack()

input_box = tk.Text(window, height=5, width=50, font=("Arial", 12))
input_box.pack(pady=10)

predict_button = tk.Button(window, text="Predict Sentiment",
                           command=predict_sentiment,
                           font=("Arial", 14, "bold"), bg="#4CAF50", fg="white")
predict_button.pack(pady=10)

result_label = tk.Label(window, text="", font=("Arial", 16, "bold"), bg="#f2f2f2")
result_label.pack(pady=20)

window.mainloop()